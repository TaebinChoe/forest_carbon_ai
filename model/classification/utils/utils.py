import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import torch
import tifffile as tiff
from torch.utils.data import Dataset
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import copy
import torch.nn as nn
import torch.nn.init as init
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu") #device ì„¤ì •

#train í•¨ìˆ˜
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, patience=10):
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = model.state_dict()  # ì´ˆê¸° ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
    no_improve_count = 0

    for epoch in range(num_epochs):
        # Training Phase
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for images, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)  # ë°°ì¹˜ë³„ loss * ê°œìˆ˜ë¡œ ì „ì²´ ì†ì‹¤ ê³„ì‚°
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_loss = running_loss / len(train_loader.dataset)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ”
        train_accuracy = 100 * correct / total
        train_losses.append(train_loss)

        # Validation Phase
        model.eval()
        val_running_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation"):
                images, labels = images.to(device), labels.to(device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                val_running_loss += loss.item() * labels.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_loss = val_running_loss / len(val_loader.dataset)
        val_accuracy = 100 * val_correct / val_total
        val_losses.append(val_loss)

        print(f"\nEpoch [{epoch+1}/{num_epochs}], "
              f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, "
              f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\n")

        # Early Stopping & Model Saving
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = copy.deepcopy(model.state_dict())
            no_improve_count = 0
        else:
            no_improve_count += 1

        if no_improve_count >= patience:
            print("Early stopping triggered. Training stopped.")
            break
    
    return best_model_state, train_losses, val_losses

#evalí•¨ìˆ˜

# í´ë˜ìŠ¤ë³„ ë¼ë²¨ ì´ë¦„ ì •ì˜
target_name_mapping = {
    0: "Non-Forest",  # ë¹„ì‚°ë¦¼
    1: "Pine",  # ì†Œë‚˜ë¬´
    2: "Nut Pine",  # ì£ë‚˜ë¬´
    3: "Larch",  # ë‚™ì—½ì†¡
    4: "Mongolian Oak",  # ì‹ ê°ˆë‚˜ë¬´
    5: "Oriental Oak"  # êµ´ì°¸ë‚˜ë¬´
}

# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
def evaluate_model_with_cm(model, val_loader, num_classes=6, target_name_mapping=target_name_mapping):
    """
    ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  í˜¼ë™ í–‰ë ¬(Confusion Matrix) ë° ë¶„ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    ì¶”ê°€ì ìœ¼ë¡œ ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ì˜ ë¶„ë¥˜ë ¥ ë° ë‚´ë¶€ ë¶„ë¥˜ë ¥ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ ë°˜í™˜í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ì— í¬í•¨í•œë‹¤.
    """
    model.eval()
    all_labels = []
    all_predictions = []

    # ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc="Evaluation Progress"):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())

    # ì „ì²´ í´ë˜ìŠ¤ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    full_class_labels = np.arange(num_classes)
    cm = confusion_matrix(all_labels, all_predictions, labels=full_class_labels)
    
    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=target_name_mapping.values(), yticklabels=target_name_mapping.values())
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()
    
    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    target_names = list(target_name_mapping.values())
    report_dict = classification_report(all_labels, all_predictions, labels=full_class_labels, target_names=target_names, digits=3, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()
    report_df["Category"] = "Overall"
    
    # ì¶”ê°€ì ì¸ ë¶„ì„ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
    conifer_vs_broadleaf_report = evaluate_conifer_vs_broadleaf(all_labels, all_predictions) #ì¹¨/í™œ ë¶„ë¥˜ë ¥
    conifer_vs_broadleaf_report["Category"] = "Conifer vs Broadleaf"
    
    conifer_report = evaluate_conifer_classification(all_labels, all_predictions) #ì¹¨ì—½ìˆ˜ ë‚´ ë¶„ë¥˜ë ¥
    conifer_report["Category"] = "Conifer"
    
    broadleaf_report = evaluate_broadleaf_classification(all_labels, all_predictions) #í™œì—½ìˆ˜ ë‚´ ë¶„ë¥˜ë ¥
    broadleaf_report["Category"] = "Broadleaf"
    
    # ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í†µí•©
    additional_metrics = pd.concat([conifer_vs_broadleaf_report, conifer_report, broadleaf_report])
    final_report_df = pd.concat([report_df, additional_metrics])
    
    return final_report_df

# ì¹¨ì—½ìˆ˜ vs í™œì—½ìˆ˜ ë¶„ë¥˜ë ¥ í‰ê°€ í•¨ìˆ˜
def evaluate_conifer_vs_broadleaf(all_labels, all_predictions):
    """
    ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ë¥¼ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.
    Non-Forestë¥¼ ì œì™¸í•œ ë°ì´í„°ì—ì„œ ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë¶„ë¥˜í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•œë‹¤.
    """
    conifer_classes = {1, 2, 3}  # ì¹¨ì—½ìˆ˜ í´ë˜ìŠ¤
    broadleaf_classes = {4, 5}  # í™œì—½ìˆ˜ í´ë˜ìŠ¤
    
    # Non-Forest ì œì™¸ í›„ ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ë¡œë§Œ ê·¸ë£¹í•‘
    valid_indices = [i for i, (label, pred) in enumerate(zip(all_labels, all_predictions)) if (label in conifer_classes or label in broadleaf_classes) and (pred in conifer_classes or pred in broadleaf_classes)]
    true_labels = [1 if all_labels[i] in conifer_classes else 2 for i in valid_indices]
    pred_labels = [1 if all_predictions[i] in conifer_classes else 2 for i in valid_indices]
    
    cm = confusion_matrix(true_labels, pred_labels, labels=[1, 2])
    
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Conifer", "Broadleaf"], yticklabels=["Conifer", "Broadleaf"])
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Conifer vs Broadleaf Classification')
    plt.show()
    
    return pd.DataFrame(classification_report(true_labels, pred_labels, target_names=["Conifer", "Broadleaf"], digits=3, output_dict=True)).transpose()

# ì¹¨ì—½ìˆ˜ ë‚´ë¶€ ë¶„ë¥˜ë ¥ í‰ê°€ í•¨ìˆ˜
def evaluate_conifer_classification(all_labels, all_predictions):
    """
    ì¹¨ì—½ìˆ˜ ë‚´ë¶€ì—ì„œ ê°œë³„ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.
    """
    conifer_classes = {1, 2, 3}
    valid_indices = [i for i, (label, pred) in enumerate(zip(all_labels, all_predictions)) if label in conifer_classes and pred in conifer_classes]
    true_labels = [all_labels[i] for i in valid_indices]
    pred_labels = [all_predictions[i] for i in valid_indices]
    
    cm = confusion_matrix(true_labels, pred_labels, labels=[1, 2, 3])
    
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Pine", "Nut Pine", "Larch"], yticklabels=["Pine", "Nut Pine", "Larch"])
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Conifer Classification')
    plt.show()
    
    return pd.DataFrame(classification_report(true_labels, pred_labels, target_names=["Pine", "Nut Pine", "Larch"], digits=3, output_dict=True)).transpose()

# í™œì—½ìˆ˜ ë‚´ë¶€ ë¶„ë¥˜ë ¥ í‰ê°€ í•¨ìˆ˜
def evaluate_broadleaf_classification(all_labels, all_predictions):
    """
    í™œì—½ìˆ˜ ë‚´ë¶€ì—ì„œ ê°œë³„ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.
    """
    broadleaf_classes = {4, 5}
    valid_indices = [i for i, (label, pred) in enumerate(zip(all_labels, all_predictions)) if label in broadleaf_classes and pred in broadleaf_classes]
    true_labels = [all_labels[i] for i in valid_indices]
    pred_labels = [all_predictions[i] for i in valid_indices]
    
    cm = confusion_matrix(true_labels, pred_labels, labels=[4, 5])
    
    plt.figure(figsize=(7, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Mongolian Oak", "Oriental Oak"], yticklabels=["Mongolian Oak", "Oriental Oak"])
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Broadleaf Classification')
    plt.show()
    
    return pd.DataFrame(classification_report(true_labels, pred_labels, target_names=["Mongolian Oak", "Oriental Oak"], digits=3, output_dict=True)).transpose()

#rfí‰ê°€ë¥¼ ìœ„í•¨
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from tqdm import tqdm

# í´ë˜ìŠ¤ë³„ ë¼ë²¨ ì´ë¦„ ì •ì˜
target_name_mapping = {
    0: "Non-Forest",  # ë¹„ì‚°ë¦¼
    1: "Pine",  # ì†Œë‚˜ë¬´
    2: "Nut Pine",  # ì£ë‚˜ë¬´
    3: "Larch",  # ë‚™ì—½ì†¡
    4: "Mongolian Oak",  # ì‹ ê°ˆë‚˜ë¬´
    5: "Oriental Oak"  # êµ´ì°¸ë‚˜ë¬´
}

# ëª¨ë¸ í‰ê°€ í•¨ìˆ˜
def evaluate_rf_model_with_cm(model, val_loader, num_classes=6, target_name_mapping=target_name_mapping):
    """
    Random Forest ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  í˜¼ë™ í–‰ë ¬(Confusion Matrix) ë° ë¶„ë¥˜ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    ì¶”ê°€ì ìœ¼ë¡œ ì¹¨ì—½ìˆ˜ì™€ í™œì—½ìˆ˜ì˜ ë¶„ë¥˜ë ¥ ë° ë‚´ë¶€ ë¶„ë¥˜ë ¥ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ ë°˜í™˜í•˜ëŠ” ë°ì´í„°í”„ë ˆì„ì— í¬í•¨í•œë‹¤.
    """
    all_labels = []
    all_predictions = []

    # ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰
    for X_batch, y_batch in tqdm(val_loader, desc="Evaluation Progress"):
        X_batch = X_batch.numpy()  # NumPyë¡œ ë³€í™˜ (RandomForestëŠ” NumPy ë°°ì—´ì„ ì‚¬ìš©)
        y_batch = y_batch.numpy()
        
        # ì˜ˆì¸¡
        preds = model.predict(X_batch)  # RandomForest ì˜ˆì¸¡

        all_labels.extend(y_batch)
        all_predictions.extend(preds)

    # ì „ì²´ í´ë˜ìŠ¤ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    full_class_labels = np.arange(num_classes)
    cm = confusion_matrix(all_labels, all_predictions, labels=full_class_labels)
    
    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=target_name_mapping.values(), yticklabels=target_name_mapping.values())
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix')
    plt.show()
    
    # ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸
    target_names = list(target_name_mapping.values())
    report_dict = classification_report(all_labels, all_predictions, labels=full_class_labels, target_names=target_names, digits=3, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()
    report_df["Category"] = "Overall"
    
    # ì¶”ê°€ì ì¸ ë¶„ì„ ìˆ˜í–‰ ë° ê²°ê³¼ ì €ì¥
    conifer_vs_broadleaf_report = evaluate_conifer_vs_broadleaf(all_labels, all_predictions)  # ì¹¨/í™œ ë¶„ë¥˜ë ¥
    conifer_vs_broadleaf_report["Category"] = "Conifer vs Broadleaf"
    
    conifer_report = evaluate_conifer_classification(all_labels, all_predictions)  # ì¹¨ì—½ìˆ˜ ë‚´ ë¶„ë¥˜ë ¥
    conifer_report["Category"] = "Conifer"
    
    broadleaf_report = evaluate_broadleaf_classification(all_labels, all_predictions)  # í™œì—½ìˆ˜ ë‚´ ë¶„ë¥˜ë ¥
    broadleaf_report["Category"] = "Broadleaf"
    
    # ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í†µí•©
    additional_metrics = pd.concat([conifer_vs_broadleaf_report, conifer_report, broadleaf_report])
    final_report_df = pd.concat([report_df, additional_metrics])
    
    return final_report_df


#dataset class
class TiffDataset(Dataset):
    def __init__(self, large_tif_dir, file_list, label_file, patch_size=3, box_filter_fn=None, transform=None):
        """
        Args:
            large_tif_dir (str): ì—¬ëŸ¬ ê°œì˜ í° TIFF íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
            file_list (list of str): ì²˜ë¦¬í•  ì›ì²œ ë°ì´í„° íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸.
            label_file (str): ë¼ë²¨ ì •ë³´ë¥¼ ë‹´ì€ CSV íŒŒì¼ ê²½ë¡œ ("file", "x_pos", "y_pos", "label" ì—´ í¬í•¨).
            patch_size (int): ìŠ¬ë¼ì´ì‹±í•  ì´ë¯¸ì§€ì˜ í¬ê¸° (í•­ìƒ í™€ìˆ˜ì—¬ì•¼ í•¨).
            box_filter_fn (callable, optional): ë°•ìŠ¤ ë²ˆí˜¸ í•„í„°ë§ í•¨ìˆ˜.
            transform (callable, optional): ì´ë¯¸ì§€ ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  í•¨ìˆ˜.
        """
        if patch_size % 2 == 0:
            raise ValueError("patch_sizeëŠ” í™€ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        self.large_tif_dir = large_tif_dir
        self.file_list = set(file_list)
        self.label_df = pd.read_csv(label_file)

        self.box_filter_fn = box_filter_fn or (lambda box_number: True)
        self.label_df = self.label_df[self.label_df['file'].isin(self.file_list)].reset_index(drop=True)
        self.label_df = self.label_df[self.label_df['box_number'].apply(self.box_filter_fn)].reset_index(drop=True)

        self.patch_size = patch_size
        self.half_size = patch_size // 2
        self.transform = transform

        self.image_cache = {}

        # ìœ íš¨í•œ ìƒ˜í”Œë§Œ ë‚¨ê¸´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        self.valid_indices = self._filter_valid_indices()

    def _load_image(self, file_name):
        if file_name not in self.image_cache:
            file_path = os.path.join(self.large_tif_dir, file_name)
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Image file '{file_path}' not found.")
            self.image_cache[file_name] = tiff.imread(file_path)
        return self.image_cache[file_name]

    def _filter_valid_indices(self):
        """ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ì €ì¥"""
        valid_indices = []
        for idx, row in self.label_df.iterrows():
            file_name = row["file"]
            x, y = row["x_pos"], row["y_pos"]

            # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
            try:
                image = self._load_image(file_name)
                image_height, image_width = image.shape[:2]

                x_start, y_start = x - self.half_size, y - self.half_size
                x_end, y_end = x + self.half_size + 1, y + self.half_size + 1

                # ì´ë¯¸ì§€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ ìœ íš¨í•œ ì¸ë±ìŠ¤ë¡œ ì €ì¥
                if 0 <= x_start and 0 <= y_start and x_end <= image_width and y_end <= image_height:
                    valid_indices.append(idx)
            except FileNotFoundError:
                continue  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ

        return valid_indices

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        row = self.label_df.iloc[self.valid_indices[idx]]
        file_name, x, y, label = row["file"], row["x_pos"], row["y_pos"], row["label"]

        image = self._load_image(file_name)
        x_start, y_start = x - self.half_size, y - self.half_size
        x_end, y_end = x + self.half_size + 1, y + self.half_size + 1

        patch = image[y_start:y_end, x_start:x_end]

        if self.transform:
            patch = self.transform(patch)

        return patch, label

    
#he initializer í•¨ìˆ˜
def he_init_weights(m):
    """
    Applies He (Kaiming) initialization to Conv layers and Linear layers.
    
    - Conv2d, Conv3d: He Normal Initialization with fan_out mode
    - Linear: He Uniform Initialization with fan_in mode
    - BatchNorm: gamma = 1, beta = 0
    - Biases (if exist): Initialized to zero
    """
    if isinstance(m, (nn.Conv2d, nn.Conv3d)):
        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)
    
    elif isinstance(m, nn.Linear):
        init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')
        if m.bias is not None:
            init.zeros_(m.bias)

    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm3d)):
        init.constant_(m.weight, 1)
        init.constant_(m.bias, 0)

#sdi í•¨ìˆ˜
def sdi_importance_analysis(model, data_loader, num_samples=1, perturbation_strength=0.2, target_dims=None):
    model.eval()

    sample_batch, _ = next(iter(data_loader))
    num_dims = len(sample_batch.shape) - 1
    dim_names = [f"dim_{i}" for i in range(1, num_dims + 1)]

    # ì¡°ì‚¬í•  ì°¨ì›ì„ ì„¤ì • (ê¸°ë³¸: ëª¨ë“  ì°¨ì›)
    if target_dims is None:
        target_dims = dim_names
    else:
        target_dims = [f"dim_{i}" for i in target_dims]

    importance_scores = {dim: 0.0 for dim in target_dims}
    per_class_scores = {}

    num_batches = 0
    for X_batch, _ in data_loader:
        num_batches += 1
        X_batch = X_batch.to(next(model.parameters()).device)

        logit_original = model(X_batch).detach()
        num_classes = logit_original.shape[1]

        if not per_class_scores:
            per_class_scores = {cls: {dim: 0.0 for dim in target_dims} for cls in range(num_classes)}

        for dim_name in target_dims:
            dim_idx = dim_names.index(dim_name) + 1  # 1-based index
            total_mse = 0.0
            class_mse = {cls: 0.0 for cls in range(num_classes)}

            for _ in range(num_samples):
                X_perturbed = X_batch.clone().detach()
                num_swap = max(1, int(X_perturbed.shape[dim_idx] * perturbation_strength))
                swap_indices = random.sample(range(X_perturbed.shape[dim_idx]), num_swap)
                permutation = random.sample(swap_indices, len(swap_indices))

                X_perturbed.index_copy_(dim_idx, torch.tensor(swap_indices, device=X_batch.device),
                                        X_perturbed.index_select(dim_idx, torch.tensor(permutation, device=X_batch.device)))

                logit_perturbed = model(X_perturbed).detach()
                mse = torch.mean((logit_original - logit_perturbed) ** 2, dim=0)
                total_mse += mse.mean().item()

                for cls in range(num_classes):
                    class_mse[cls] += mse[cls].item()

            importance_scores[dim_name] += total_mse / num_samples
            for cls in range(num_classes):
                per_class_scores[cls][dim_name] += class_mse[cls] / num_samples

    for key in importance_scores:
        importance_scores[key] /= num_batches

    for cls in per_class_scores:
        for key in per_class_scores[cls]:
            per_class_scores[cls][key] /= num_batches

    # ğŸ”¹ ì„ íƒí•œ ì°¨ì›ë§Œ ì •ê·œí™”í•˜ì—¬ í•©ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •
    total_score = sum(importance_scores.values())
    importance_scores = {key: value / total_score for key, value in importance_scores.items()} if total_score > 0 else importance_scores

    for cls in per_class_scores:
        class_total_score = sum(per_class_scores[cls].values())
        per_class_scores[cls] = {key: value / class_total_score for key, value in per_class_scores[cls].items()} if class_total_score > 0 else per_class_scores[cls]

    return {"overall": importance_scores, "per_class": per_class_scores}


def plot_importance_scores(importance_scores, per_class_scores):
    # Overall Dimension Importance (Bar Chart)
    plt.figure(figsize=(8, 5))
    plt.bar(importance_scores.keys(), importance_scores.values(), color='skyblue')
    plt.xlabel("Dimension")
    plt.ylabel("Importance Score")
    plt.title("Overall Dimension Importance")
    plt.xticks(rotation=45)
    plt.show()

    # Per-Class Importance (Heatmap)
    per_class_df = {cls: list(scores.values()) for cls, scores in per_class_scores.items()}
    dim_labels = list(per_class_scores[0].keys())  # Dimension names

    plt.figure(figsize=(10, 6))
    sns.heatmap(list(per_class_df.values()), annot=True, cmap="Blues", xticklabels=dim_labels, yticklabels=list(per_class_df.keys()))
    plt.xlabel("Dimension")
    plt.ylabel("Class")
    plt.title("Per-Class Dimension Importance")
    plt.show()