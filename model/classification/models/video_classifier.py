# import torch
# import torch.nn as nn

# """
# video ì²˜ëŸ¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ë¸
# """
# # SwigLU í™œì„± í•¨ìˆ˜ ì •ì˜ (SiLU ë³€í˜•)
# class SwigLU(nn.Module):
#     def forward(self, x):
#         return x * torch.sigmoid(1.3 * x)

# # 2+1D ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡: ê³µê°„ê³¼ ì‹œê°„ ì¶•ì„ ë¶„ë¦¬í•˜ì—¬ ì»¨ë³¼ë£¨ì…˜ ìˆ˜í–‰
# class Conv2Plus1D(nn.Module):
#     def __init__(self, in_channels, out_channels, kernel_size=(3,3,3), stride=1, padding=1):
#         super().__init__()
#         mid_channels = out_channels  # ì¤‘ê°„ ì±„ë„ ìˆ˜ ì„¤ì •
        
#         # ê³µê°„ ì°¨ì›ì— ëŒ€í•œ 2D ì»¨ë³¼ë£¨ì…˜
#         self.spatial_conv = nn.Conv3d(in_channels, mid_channels, kernel_size=(1, kernel_size[1], kernel_size[2]), 
#                                       stride=(1, stride, stride), padding=(0, padding, padding), bias=False)
#         self.bn1 = nn.BatchNorm3d(mid_channels)
        
#         # ì‹œê°„ ì°¨ì›ì— ëŒ€í•œ 1D ì»¨ë³¼ë£¨ì…˜
#         self.temporal_conv = nn.Conv3d(mid_channels, out_channels, kernel_size=(kernel_size[0], 1, 1), 
#                                        stride=(stride, 1, 1), padding=(padding, 0, 0), bias=False)
#         self.bn2 = nn.BatchNorm3d(out_channels)
#         self.relu = nn.ReLU(inplace=True)
        
#     def forward(self, x):
#         x = self.relu(self.bn1(self.spatial_conv(x)))
#         x = self.relu(self.bn2(self.temporal_conv(x)))
#         return x

# # ResNet ìŠ¤íƒ€ì¼ì˜ ê¸°ë³¸ ë¸”ë¡ (Skip Connection í¬í•¨)
# class ResBlock(nn.Module):
#     def __init__(self, channels):
#         super().__init__()
#         self.conv1 = Conv2Plus1D(channels, channels)
#         self.conv2 = Conv2Plus1D(channels, channels)
#         self.bn = nn.BatchNorm3d(channels)
#         self.relu = nn.ReLU(inplace=True)
    
#     def forward(self, x):
#         residual = x  # ì›ë³¸ ì…ë ¥ ì €ì¥
#         x = self.conv1(x)
#         x = self.conv2(x)
#         x = self.bn(x)
#         return self.relu(x + residual)  # Skip Connection ì ìš©

# # ë°´ë“œ(ì±„ë„) í™•ì¥ ë¸”ë¡: 1x1 ì»¨ë³¼ë£¨ì…˜ ì‚¬ìš©
# class BandExpansion(nn.Module):
#     def __init__(self, in_channels, out_channels):
#         super().__init__()
#         self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)
#         self.bn = nn.BatchNorm3d(out_channels)
#         self.relu = nn.ReLU(inplace=True)
    
#     def forward(self, x):
#         return self.relu(self.bn(self.conv(x)))

# # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡: SwigLU í™œì„± í•¨ìˆ˜ì™€ í™•ì¥ ë¹„ìœ¨ ì ìš©
# class FeedForward(nn.Module):
#     def __init__(self, channels, expand_ratio=2):
#         super().__init__()
#         hidden_dim = channels * expand_ratio  # í™•ì¥ëœ ì±„ë„ í¬ê¸°
#         self.fc1 = nn.Conv3d(channels, hidden_dim, kernel_size=1)
#         self.act = SwigLU()
#         self.fc2 = nn.Conv3d(hidden_dim, channels, kernel_size=1)
#         self.bn = nn.BatchNorm3d(channels)
    
#     def forward(self, x):
#         return self.bn(self.fc2(self.act(self.fc1(x))))

# # ì „ì²´ ë¹„ë””ì˜¤ ë¶„ë¥˜ ëª¨ë¸
# class VideoClassifier(nn.Module):
#     def __init__(self, input_bands, stage_repeats, stage_channels, num_classes=6):
#         super().__init__()
        
#         # ì´ˆê¸° ë°´ë“œ í™•ì¥ (ì…ë ¥ ë°´ë“œ ìˆ˜ -> ì²« ë²ˆì§¸ stage ì±„ë„ í¬ê¸°)
#         self.initial_conv = BandExpansion(input_bands, stage_channels[0])
        
#         self.stages = nn.ModuleList()
#         in_channels = stage_channels[0]
        
#         # 4ê°œì˜ Stage êµ¬ì„±
#         for stage_idx in range(4):
#             # ê¸°ë³¸ ResBlock ë°˜ë³µ ì ìš©
#             blocks = [ResBlock(in_channels) for _ in range(stage_repeats[stage_idx])]
#             self.stages.append(nn.Sequential(*blocks))
            
#             # ë§ˆì§€ë§‰ Stageë¥¼ ì œì™¸í•˜ê³  ë°´ë“œ í™•ì¥ ìˆ˜í–‰
#             if stage_idx < 3:
#                 self.stages.append(BandExpansion(in_channels, stage_channels[stage_idx + 1]))
#                 in_channels = stage_channels[stage_idx + 1]
        
#         # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ ì ìš© (ê³µê°„ ì°¨ì› ì œê±°)
#         self.gap = nn.AdaptiveAvgPool3d(1)
        
#         # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ 2ê°œ ì¶”ê°€
#         self.ff1 = FeedForward(in_channels)
#         self.ff2 = FeedForward(in_channels)
        
#         # ìµœì¢… ë¶„ë¥˜ê¸° (Linear ë ˆì´ì–´)
#         self.classifier = nn.Linear(in_channels, num_classes)
        
#     def forward(self, x):
#         x = self.initial_conv(x)  # ì´ˆê¸° ë°´ë“œ í™•ì¥ ì ìš©
        
#         # Stage ë°˜ë³µ ìˆ˜í–‰
#         for stage in self.stages:
#             x = stage(x)
        
#         # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ ì ìš© í›„ ì°¨ì› ì¶•ì†Œ
#         x = self.gap(x).view(x.size(0), -1)
        
#         # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ ë‘ ê°œ í†µê³¼
#         x = self.ff1(x.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1))
#         x = self.ff2(x)
        
#         # ìµœì¢… ë¶„ë¥˜ê¸° ì ìš©
#         x = self.classifier(x.squeeze(-1).squeeze(-1).squeeze(-1))
#         return x

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
"""
ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ (Batch, Channels, Temp, Height, Width) í˜•ì‹ìœ¼ë¡œ ì…ë ¥ë°›ì•„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸
ì‹œê°„ ì°¨ì›ì˜ ì¤‘ìš”ì„±ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ Conv2Plus1Dì—ì„œ ì‹œê°„ ì¶•ì„ ë¨¼ì € ì²˜ë¦¬í•œ í›„ ê³µê°„ ì •ë³´ë¥¼ í•™ìŠµí•¨
Dropoutì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì„
"""

# SwigLU í™œì„± í•¨ìˆ˜ ì •ì˜ (SiLU ë³€í˜•)
class SwigLU(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(1.3 * x)

# 2+1D ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡: ì‹œê°„ ì¶•ì„ ë¨¼ì € ì²˜ë¦¬í•œ í›„ ê³µê°„ ì°¨ì›ì„ í•™ìŠµ
class Conv2Plus1D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=(3,3,3), stride=1, padding=1, dropout=0.3):
        super().__init__()
        mid_channels = out_channels  # ì¤‘ê°„ ì±„ë„ ìˆ˜ ì„¤ì •
        
        # ì‹œê°„ ì°¨ì›ì— ëŒ€í•œ 1D ì»¨ë³¼ë£¨ì…˜
        self.temporal_conv = nn.Conv3d(in_channels, mid_channels, kernel_size=(kernel_size[0], 1, 1), 
                                       stride=(stride, 1, 1), padding=(padding, 0, 0), bias=False)
        self.bn1 = nn.BatchNorm3d(mid_channels)
        self.relu = nn.ReLU(inplace=True)
        self.dropout1 = nn.Dropout3d(dropout)
        
        # ê³µê°„ ì°¨ì›ì— ëŒ€í•œ 2D ì»¨ë³¼ë£¨ì…˜
        self.spatial_conv = nn.Conv3d(mid_channels, out_channels, kernel_size=(1, kernel_size[1], kernel_size[2]), 
                                      stride=(1, stride, stride), padding=(0, padding, padding), bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)
        self.dropout2 = nn.Dropout3d(dropout)
        
    def forward(self, x):
        x = self.relu(self.bn1(self.temporal_conv(x)))
        x = self.dropout1(x)
        x = self.relu(self.bn2(self.spatial_conv(x)))
        x = self.dropout2(x)
        return x

# ResNet ìŠ¤íƒ€ì¼ì˜ ê¸°ë³¸ ë¸”ë¡ (Skip Connection í¬í•¨)
class ResBlock(nn.Module):
    def __init__(self, channels, dropout=0.3):
        super().__init__()
        self.conv1 = Conv2Plus1D(channels, channels, dropout=dropout)
        self.conv2 = Conv2Plus1D(channels, channels, dropout=dropout)
        self.bn = nn.BatchNorm3d(channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        residual = x  # ì›ë³¸ ì…ë ¥ ì €ì¥
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.bn(x)
        return self.relu(x + residual)  # Skip Connection ì ìš©

# ë°´ë“œ(ì±„ë„) í™•ì¥ ë¸”ë¡: 1x1 ì»¨ë³¼ë£¨ì…˜ ì‚¬ìš©
class BandExpansion(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm3d(out_channels)
        self.relu = nn.ReLU(inplace=True)
    
    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))

# í”¼ë“œí¬ì›Œë“œ ë¸”ë¡: SwigLU í™œì„± í•¨ìˆ˜ì™€ í™•ì¥ ë¹„ìœ¨ ì ìš©
class FeedForward(nn.Module):
    def __init__(self, channels, expand_ratio=2, dropout=0.3):
        super().__init__()
        hidden_dim = channels * expand_ratio  # í™•ì¥ëœ ì±„ë„ í¬ê¸°
        self.fc1 = nn.Conv3d(channels, hidden_dim, kernel_size=1)
        self.act = SwigLU()
        self.fc2 = nn.Conv3d(hidden_dim, channels, kernel_size=1)
        self.bn = nn.BatchNorm3d(channels)
        self.dropout = nn.Dropout3d(dropout)
    
    def forward(self, x):
        return self.dropout(self.bn(self.fc2(self.act(self.fc1(x)))))

# ì „ì²´ ë¹„ë””ì˜¤ ë¶„ë¥˜ ëª¨ë¸
class VideoClassifier(nn.Module):
    def __init__(self, input_bands, stage_repeats, stage_channels, num_classes=6, dropout=0.3):
        super().__init__()
        
        # ì´ˆê¸° ë°´ë“œ í™•ì¥ (ì…ë ¥ ë°´ë“œ ìˆ˜ -> ì²« ë²ˆì§¸ stage ì±„ë„ í¬ê¸°)
        self.initial_conv = BandExpansion(input_bands, stage_channels[0])
        
        self.stages = nn.ModuleList()
        in_channels = stage_channels[0]
        
        # 4ê°œì˜ Stage êµ¬ì„±
        for stage_idx in range(4):
            # ê¸°ë³¸ ResBlock ë°˜ë³µ ì ìš©
            blocks = [ResBlock(in_channels, dropout) for _ in range(stage_repeats[stage_idx])]
            self.stages.append(nn.Sequential(*blocks))
            
            # ë§ˆì§€ë§‰ Stageë¥¼ ì œì™¸í•˜ê³  ë°´ë“œ í™•ì¥ ìˆ˜í–‰
            if stage_idx < 3:
                self.stages.append(BandExpansion(in_channels, stage_channels[stage_idx + 1]))
                in_channels = stage_channels[stage_idx + 1]
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ ì ìš© (ê³µê°„ ì°¨ì› ì œê±°)
        self.gap = nn.AdaptiveAvgPool3d(1)
        
        # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ 2ê°œ ì¶”ê°€
        self.ff1 = FeedForward(in_channels, dropout=dropout)
        self.ff2 = FeedForward(in_channels, dropout=dropout)
        
        # ìµœì¢… ë¶„ë¥˜ê¸° (Linear ë ˆì´ì–´)
        self.classifier = nn.Linear(in_channels, num_classes)
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì ìš©
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):
            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            if m.bias is not None:
                init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.initial_conv(x)  # ì´ˆê¸° ë°´ë“œ í™•ì¥ ì ìš©
        
        # Stage ë°˜ë³µ ìˆ˜í–‰
        for stage in self.stages:
            x = stage(x)
        
        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§ ì ìš© í›„ ì°¨ì› ì¶•ì†Œ
        x = self.gap(x).view(x.size(0), -1)
        
        # í”¼ë“œí¬ì›Œë“œ ë¸”ë¡ ë‘ ê°œ í†µê³¼
        x = self.ff1(x.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1))
        x = self.ff2(x)
        
        # ìµœì¢… ë¶„ë¥˜ê¸° ì ìš©
        x = self.classifier(x.squeeze(-1).squeeze(-1).squeeze(-1))
        return x
#(Token_length, embedding) = (12, 4Ã—81)ì¸ transformer ëª¨ë¸

class TransformerModel_(nn.Module):
    def __init__(self, patch_size=9, num_bands=4, temp=12, num_classes=6, 
                 d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):
        super().__init__()
        self.patch_size = patch_size
        self.num_bands = num_bands
        self.temp = temp
        self.num_tokens = temp  # 12ê°œ (ì‹œê³„ì—´ ë‹¨ìœ„)
        self.d_model = d_model  

        # ** ì…ë ¥ ì°¨ì› ë³€í™˜ (4Ã—9Ã—9 â†’ d_model) **
        self.input_projection = nn.Linear(num_bands * patch_size * patch_size, d_model)

        # Positional Encoding
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))

        # ** Layer Normalization ì¶”ê°€ **
        self.norm1 = nn.LayerNorm(d_model)  

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,  
            dim_feedforward=dim_feedforward,  
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Classification Head (Dropout ì¶”ê°€)
        self.fc = nn.Sequential(
            nn.Linear(self.num_tokens * d_model, 128),  # 192 â†’ 128
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),  # 96 â†’ 64
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

        # ** Layer Normalization ì¶”ê°€ **
        self.norm2 = nn.LayerNorm(num_classes)  

        # ** ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” **
        self.apply(self._init_weights)  

    def forward(self, x):
        batch_size = x.shape[0]

        # (batch, 4, 12, 9, 9) â†’ (batch, 12, 4Ã—9Ã—9)
        x = x.permute(0, 2, 1, 3, 4).reshape(batch_size, self.num_tokens, -1)

        # ì…ë ¥ ì°¨ì› ë³€í™˜ (4Ã—9Ã—9 â†’ d_model)
        x = self.input_projection(x)

        # Add Positional Encoding
        x = x + self.pos_embedding  

        # Transformer Encoder
        x = self.norm1(x)  
        x = self.transformer(x)  

        # Flatten & Classification
        x = x.flatten(1)  
        x = self.fc(x)

        # ì¶œë ¥ ì •ê·œí™”
        x = self.norm2(x)  

        return x

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)  
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

#ìƒˆë¡œìš´ transformer ëª¨ë¸
import torch
import torch.nn as nn

class CustomTransformerEncoderLayer(nn.TransformerEncoderLayer):
    def forward(self, src, src_mask=None, src_key_padding_mask=None, return_attention=False, **kwargs):
        """Residual Connection í›„ LayerNorm ì ìš©"""
        
        # Self-Attention Layer
        src2, attn_weights = self.self_attn(
            src, src, src, attn_mask=src_mask, 
            key_padding_mask=src_key_padding_mask, need_weights=True
        )  
        src = self.norm1(src + self.dropout1(src2))  # ğŸ”¹ Residual í›„ LayerNorm
        
        # Feedforward Layer
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = self.norm2(src + self.dropout2(src2))  # ğŸ”¹ Residual í›„ LayerNorm
        
        if return_attention:
            return src, attn_weights
        return src

class CustomTransformerEncoder(nn.Module):
    def __init__(self, d_model=64, num_layers=4, nhead=4, dim_feedforward=128, dropout=0.1):
        super().__init__()
        encoder_layer = CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, src, return_attention=False):
        if return_attention:
            outputs, attn_weights = [], []
            for layer in self.encoder.layers:
                src, attn = layer(src, return_attention=True)
                attn_weights.append(attn)
                outputs.append(src)

            return outputs[-1], torch.stack(attn_weights, dim=1)  
        return self.encoder(src)

class TransformerModel(nn.Module):
    def __init__(self, patch_size=9, num_bands=4, temp=12, num_classes=6, 
                 d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):
        super().__init__()
        self.patch_size = patch_size
        self.num_bands = num_bands
        self.temp = temp
        self.num_tokens = temp
        self.d_model = d_model  

        # **ì…ë ¥ ì°¨ì› ë³€í™˜ í›„ LayerNorm ì¶”ê°€**
        self.input_projection = nn.Linear(num_bands * patch_size * patch_size, d_model)
        self.norm_input = nn.LayerNorm(d_model)  # ğŸ”¹ ì…ë ¥ ì •ê·œí™” ì¶”ê°€

        # Positional Encoding
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))

        # Transformer Encoder
        self.transformer = CustomTransformerEncoder(d_model, num_layers, nhead, dim_feedforward, dropout)

        # Classification Head (Dropout ê°•í™”)
        self.fc = nn.Sequential(
            nn.Linear(self.num_tokens * d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

        self.norm_out = nn.LayerNorm(num_classes)  # ğŸ”¹ ì¶œë ¥ ì •ê·œí™” ì¶”ê°€

        # **ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”**
        self.apply(self._init_weights)  

    def forward(self, x, return_attention=False):
        batch_size = x.shape[0]

        # (batch, 4, 12, 9, 9) â†’ (batch, 12, 4Ã—9Ã—9)
        x = x.permute(0, 2, 1, 3, 4).reshape(batch_size, self.num_tokens, -1)

        # ì…ë ¥ ì°¨ì› ë³€í™˜ & LayerNorm ì ìš©
        x = self.input_projection(x)
        x = self.norm_input(x)  # ğŸ”¹ ì…ë ¥ ì •ê·œí™”

        # Positional Encoding ì¶”ê°€
        x = x + self.pos_embedding  

        # Transformer Encoder
        if return_attention:
            x, attn_weights = self.transformer(x, return_attention=True)
        else:
            x = self.transformer(x)

        # Flatten & Classification
        x = x.flatten(1)  
        x = self.fc(x)

        # ì¶œë ¥ ì •ê·œí™”
        x = self.norm_out(x)  

        if return_attention:
            return x, attn_weights  
        return x

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_normal_(module.weight)  # ğŸ”¹ Xavier Normal ì ìš©
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)

# ğŸ”¹ Gradient Clipping ì ìš© (í›ˆë ¨ ë£¨í”„ì—ì„œ ì¶”ê°€)
def train_model(model, dataloader, optimizer, criterion, clip_value=1.0):
    model.train()
    for batch in dataloader:
        inputs, targets = batch
        optimizer.zero_grad()
        
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)  # ğŸ”¹ Gradient Clipping
        optimizer.step()

# class CustomTransformerEncoderLayer(nn.TransformerEncoderLayer):
#     def forward(self, src, src_mask=None, src_key_padding_mask=None, return_attention=False, **kwargs):
#         """Attention ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì • + is_causal ì¸ì ë¬´ì‹œ"""
        
#         # `is_causal` ê°™ì€ ì¶”ê°€ ì¸ìë¥¼ ë¬´ì‹œí•˜ë„ë¡ kwargs ì²˜ë¦¬
#         src2, attn_weights = self.self_attn(
#             src, src, src, attn_mask=src_mask, 
#             key_padding_mask=src_key_padding_mask, need_weights=True
#         )  
#         src = src + self.dropout1(src2)
#         src = self.norm1(src)
#         src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
#         src = src + self.dropout2(src2)
#         src = self.norm2(src)
        
#         if return_attention:
#             return src, attn_weights  # ğŸ”¹ Attention ê°€ì¤‘ì¹˜ ë°˜í™˜
#         else:
#             return src

# class CustomTransformerEncoder(nn.Module):
#     def __init__(self, d_model=64, num_layers=4, nhead=4, dim_feedforward=128, dropout=0.1):
#         super().__init__()
#         encoder_layer = CustomTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

#     def forward(self, src, return_attention=False):
#         """Custom Transformer Encoder: Attention ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •"""
#         if return_attention:
#             outputs, attn_weights = [], []
#             for layer in self.encoder.layers:
#                 src, attn = layer(src, return_attention=True)
#                 attn_weights.append(attn)
#                 outputs.append(src)

#             return outputs[-1], torch.stack(attn_weights, dim=1)  # (batch, num_layers, seq_len, seq_len)
#         else:
#             return self.encoder(src)

# class TransformerModel(nn.Module):
#     def __init__(self, patch_size=9, num_bands=4, temp=12, num_classes=6, 
#                  d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):
#         super().__init__()
#         self.patch_size = patch_size
#         self.num_bands = num_bands
#         self.temp = temp
#         self.num_tokens = temp  # 12ê°œ (ì‹œê³„ì—´ ë‹¨ìœ„)
#         self.d_model = d_model  

#         # ** ì…ë ¥ ì°¨ì› ë³€í™˜ (4Ã—9Ã—9 â†’ d_model) **
#         self.input_projection = nn.Linear(num_bands * patch_size * patch_size, d_model)

#         # Positional Encoding
#         self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))

#         # ** Layer Normalization ì¶”ê°€ **
#         self.norm1 = nn.LayerNorm(d_model)  

#         # Transformer Encoder (Custom ë²„ì „ ì ìš©)
#         self.transformer = CustomTransformerEncoder(d_model, num_layers, nhead, dim_feedforward, dropout)

#         # Classification Head (Dropout ì¶”ê°€)
#         self.fc = nn.Sequential(
#             nn.Linear(self.num_tokens * d_model, 128),  # 192 â†’ 128
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(128, 64),  # 96 â†’ 64
#             nn.ReLU(),
#             nn.Dropout(0.3),
#             nn.Linear(64, num_classes)
#         )

#         # ** Layer Normalization ì¶”ê°€ **
#         self.norm2 = nn.LayerNorm(num_classes)  

#         # ** ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” **
#         self.apply(self._init_weights)  

#     def forward(self, x, return_attention=False):
#         batch_size = x.shape[0]

#         # (batch, 4, 12, 9, 9) â†’ (batch, 12, 4Ã—9Ã—9)
#         x = x.permute(0, 2, 1, 3, 4).reshape(batch_size, self.num_tokens, -1)

#         # ì…ë ¥ ì°¨ì› ë³€í™˜ (4Ã—9Ã—9 â†’ d_model)
#         x = self.input_projection(x)

#         # Add Positional Encoding
#         x = x + self.pos_embedding  

#         # Transformer Encoder
#         x = self.norm1(x)  
#         if return_attention:
#             x, attn_weights = self.transformer(x, return_attention=True)
#         else:
#             x = self.transformer(x)

#         # Flatten & Classification
#         x = x.flatten(1)  
#         x = self.fc(x)

#         # ì¶œë ¥ ì •ê·œí™”
#         x = self.norm2(x)  

#         if return_attention:
#             return x, attn_weights  # ğŸ”¹ Attention ê°€ì¤‘ì¹˜ë„ ë°˜í™˜
#         return x

#     def _init_weights(self, module):
#         if isinstance(module, nn.Linear):
#             nn.init.xavier_uniform_(module.weight)  
#             if module.bias is not None:
#                 nn.init.constant_(module.bias, 0)

                
#hybrid ëª¨ë¸            
class HybridCNNTransformer(nn.Module):
    def __init__(self, patch_size=9, num_bands=4, temp=12, num_classes=6, 
                 d_model=64, nhead=4, num_layers=4, dim_feedforward=128, dropout=0.1):
        super().__init__()
        self.patch_size = patch_size
        self.num_bands = num_bands
        self.temp = temp
        self.num_tokens = temp  # 12ê°œ (ì‹œê³„ì—´ ë‹¨ìœ„)
        self.d_model = d_model  

        # ** 2+1D CNN Feature Extractor **
        self.cnn_feature_extractor = nn.Sequential(
            nn.Conv3d(in_channels=num_bands, out_channels=32, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.ReLU(),
            nn.Conv3d(in_channels=32, out_channels=d_model, kernel_size=(3, 1, 1), padding=(1, 0, 0)),
            nn.ReLU(),
            nn.AdaptiveAvgPool3d((self.num_tokens, 1, 1))  # (B, d_model, 12, 1, 1)
        )

        # Positional Encoding
        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, d_model))

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Classification Head
        self.fc = nn.Sequential(
            nn.Linear(self.num_tokens * d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes)
        )

        self.norm2 = nn.LayerNorm(num_classes)

        self.apply(self._init_weights)

    def forward(self, x):
        batch_size = x.shape[0]

        # CNN Feature Extraction (B, 4, 12, 9, 9) -> (B, d_model, 12, 1, 1)
        x = self.cnn_feature_extractor(x)
        x = x.squeeze(-1).squeeze(-1)  # (B, d_model, 12)
        x = x.permute(0, 2, 1)  # (B, 12, d_model)

        # Add Positional Encoding
        x = x + self.pos_embedding

        # Transformer Encoder
        x = self.norm1(x)
        x = self.transformer(x)

        # Flatten & Classification
        x = x.flatten(1)
        x = self.fc(x)
        x = self.norm2(x)

        return x

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)

