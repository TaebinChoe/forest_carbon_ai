{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import train_model, base_transform, evaluate_model_with_cm, TiffDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import copy\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"../../data/label_data/species/label_mapping_sampled.csv\")\n",
    "df_test = pd.read_csv(\"../../data/label_data/species/label_mapping_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['type'] = 'train'\n",
    "df_test['type'] = 'test'\n",
    "\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=6, kernel_size=3):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        # ResNet18 ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©\n",
    "        self.transform = transform\n",
    "        self.resnet = models.resnet18(weights=None)  # ì‚¬ì „ í›ˆë ¨ ì—†ì´ ì´ˆê¸°í™”\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=kernel_size, stride=1, padding=1, bias=False)  # ì…ë ¥ ì±„ë„ 9ê°œë¡œ ë³€ê²½\n",
    "        self.resnet.maxpool = nn.Identity()  # 5x5 ì…ë ¥ì´ë¯€ë¡œ MaxPooling ì œê±°\n",
    "        self.resnet.fc = nn.Linear(512, num_classes)  # ë§ˆì§€ë§‰ ì¶œë ¥ ë‰´ëŸ°ì„ í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ ë³€ê²½\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, dim, expand_ratio=4, dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        hidden_dim = dim * expand_ratio\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, expand_ratio=4, dropout=0.1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.ffn = FeedForwardBlock(dim, expand_ratio, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.norm(x)\n",
    "        out = self.ffn(out)\n",
    "        return x + out  # Skip Connection\n",
    "\n",
    "class DNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=10, hidden_dim=128, output_dim=6, num_layers=10, expand_ratio=4, dropout=0.1):\n",
    "        super(DNNClassifier, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layers = nn.Sequential(*[ResidualBlock(hidden_dim, expand_ratio, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=10, embed_dim=16, num_heads=2, num_layers=2, num_classes=6, seq_len=12, dropout=0.1):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        # Linear layer for embedding input_dim -> embed_dim\n",
    "        self.input_fc = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Transformer Encoder Layer ì„¤ì • (batch_first=True)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,  # ì…ë ¥ì˜ ì„ë² ë”© ì°¨ì›\n",
    "            nhead=num_heads,  # Multi-head Attentionì˜ head ê°œìˆ˜\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # ë°°ì¹˜ ì°¨ì›ì´ ì²« ë²ˆì§¸ë¡œ ì˜¤ë„ë¡ ì„¤ì •\n",
    "        )\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers  # Encoder Layerì˜ ìˆ˜\n",
    "        )\n",
    "        \n",
    "        # ë§ˆì§€ë§‰ ì¶œë ¥ì—ì„œ ë¶„ë¥˜ë¥¼ ìœ„í•œ Linear Layer\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        #ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        self._init_weights()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # ì…ë ¥ì„ ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "        x = self.input_fc(x)\n",
    "        \n",
    "        # Transformer Encoder í†µê³¼\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "        \n",
    "        # transformer_outì˜ ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì¶œë ¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        # (batch, seq_len, embed_dim) -> (batch, embed_dim)\n",
    "        out = transformer_out[:, -1, :]\n",
    "        \n",
    "        # FC Layerë¥¼ í†µí•œ ë¶„ë¥˜\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.TransformerEncoderLayer):\n",
    "                nn.init.xavier_uniform_(m.linear1.weight)\n",
    "                nn.init.xavier_uniform_(m.linear2.weight)\n",
    "                if m.linear1.bias is not None:\n",
    "                    nn.init.zeros_(m.linear1.bias)\n",
    "                if m.linear2.bias is not None:\n",
    "                    nn.init.zeros_(m.linear2.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def get_gaussian_kernel(kernel_size, sigma):\n",
    "    \"\"\"\n",
    "    2D Gaussian Kernel ìƒì„±\n",
    "    \"\"\"\n",
    "    ax = torch.arange(kernel_size, dtype=torch.float32) - (kernel_size - 1) / 2\n",
    "    xx, yy = torch.meshgrid(ax, ax, indexing='ij')\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    kernel /= kernel.sum()\n",
    "    return kernel\n",
    "\n",
    "class DownsampleWithGaussian:\n",
    "    def __init__(self, kernel_size: int, sigma: float = None):\n",
    "        \"\"\"\n",
    "        kernel_size: ê°€ìš°ì‹œì•ˆ í•„í„° í¬ê¸°\n",
    "        sigma: ê°€ìš°ì‹œì•ˆ í•„í„°ì˜ í‘œì¤€í¸ì°¨ (Noneì¼ ê²½ìš° ê¸°ë³¸ê°’ ìë™ ì„¤ì •)\n",
    "        \"\"\"\n",
    "        self.kernel_size = kernel_size\n",
    "        if sigma is None:\n",
    "            sigma = 0.3 * ((kernel_size - 1) * 0.5 - 1) + 0.8  # ì¼ë°˜ì ì¸ ë””í´íŠ¸ ê°’\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # ê°€ìš°ì‹œì•ˆ ì»¤ë„ ìƒì„±\n",
    "        kernel = get_gaussian_kernel(kernel_size, sigma)\n",
    "        self.kernel = kernel.view(1, 1, kernel_size, kernel_size)  # (1, 1, K, K)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (B, T, H, W)\n",
    "        Returns: Downsampled Tensor of shape (B, T, H//kernel_size, W//kernel_size)\n",
    "        \"\"\"\n",
    "        B, T, H, W = x.shape\n",
    "  \n",
    "        if H % self.kernel_size != 0 or W % self.kernel_size != 0:\n",
    "            raise ValueError(f\"Input size ({H}, {W}) must be divisible by kernel_size {self.kernel_size}\")\n",
    "        \n",
    "        # ì±„ë„ ì°¨ì› ì¶”ê°€ í›„ Gaussian Blur ì ìš©\n",
    "        x = x.contiguous().view(B * T, 1, H, W)  # (B*T, 1, H, W)\n",
    "        x_blurred = F.conv2d(x, self.kernel.to(x.device), stride=self.kernel_size, padding=0)  # (B*T, 1, H//K, W//K)\n",
    "        \n",
    "        return x_blurred.view(B, T, H // self.kernel_size, W // self.kernel_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, scheduler=None):\n",
    "    \n",
    "    if patience == None:\n",
    "        patience= num_epochs\n",
    "        \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_f1_score = 0\n",
    "    best_model_state = model.state_dict()  # ì´ˆê¸° ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_running_loss = 0.0\n",
    "        \n",
    "        train_labels = []\n",
    "        train_predictions = []\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.item() * labels.size(0)  # ë°°ì¹˜ë³„ loss * ê°œìˆ˜ë¡œ ì „ì²´ ì†ì‹¤ ê³„ì‚°\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "        train_loss = train_running_loss / len(train_loader.dataset)  # ì „ì²´ ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ”\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        full_class_labels = np.arange(outputs.shape[1])\n",
    "        train_report = classification_report(train_labels, train_predictions, labels=full_class_labels, output_dict=True)\n",
    "        \n",
    "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_report['accuracy']:.2f}, Train f1-score: {train_report[\"macro avg\"][\"f1-score\"]:.2f} \")       \n",
    "       \n",
    "        if scheduler:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)  # validation loss ê¸°ì¤€\n",
    "            else:\n",
    "                scheduler.step()  # ì¼ë°˜ì ì¸ step()\n",
    "    \n",
    "    return best_model_state, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>x_pos</th>\n",
       "      <th>y_pos</th>\n",
       "      <th>file</th>\n",
       "      <th>box_number</th>\n",
       "      <th>tree_species</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1060</td>\n",
       "      <td>1063</td>\n",
       "      <td>jiri_1.tif</td>\n",
       "      <td>23</td>\n",
       "      <td>QM</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>463</td>\n",
       "      <td>997</td>\n",
       "      <td>jiri_1.tif</td>\n",
       "      <td>22</td>\n",
       "      <td>QM</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>889</td>\n",
       "      <td>1147</td>\n",
       "      <td>jiri_1.tif</td>\n",
       "      <td>33</td>\n",
       "      <td>QM</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>652</td>\n",
       "      <td>298</td>\n",
       "      <td>jiri_1.tif</td>\n",
       "      <td>2</td>\n",
       "      <td>QM</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>388</td>\n",
       "      <td>634</td>\n",
       "      <td>jiri_1.tif</td>\n",
       "      <td>12</td>\n",
       "      <td>QM</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14394</th>\n",
       "      <td>5</td>\n",
       "      <td>1078</td>\n",
       "      <td>568</td>\n",
       "      <td>sobaek.tif</td>\n",
       "      <td>13</td>\n",
       "      <td>QV</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>5</td>\n",
       "      <td>1126</td>\n",
       "      <td>505</td>\n",
       "      <td>sobaek.tif</td>\n",
       "      <td>14</td>\n",
       "      <td>QV</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14396</th>\n",
       "      <td>5</td>\n",
       "      <td>190</td>\n",
       "      <td>430</td>\n",
       "      <td>sobaek.tif</td>\n",
       "      <td>11</td>\n",
       "      <td>QV</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14397</th>\n",
       "      <td>5</td>\n",
       "      <td>3535</td>\n",
       "      <td>751</td>\n",
       "      <td>sobaek.tif</td>\n",
       "      <td>30</td>\n",
       "      <td>QV</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>5</td>\n",
       "      <td>1393</td>\n",
       "      <td>244</td>\n",
       "      <td>sobaek.tif</td>\n",
       "      <td>4</td>\n",
       "      <td>QV</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242500 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  x_pos  y_pos        file  box_number tree_species   type\n",
       "0          4   1060   1063  jiri_1.tif          23           QM  train\n",
       "1          4    463    997  jiri_1.tif          22           QM  train\n",
       "2          4    889   1147  jiri_1.tif          33           QM  train\n",
       "3          4    652    298  jiri_1.tif           2           QM  train\n",
       "4          4    388    634  jiri_1.tif          12           QM  train\n",
       "...      ...    ...    ...         ...         ...          ...    ...\n",
       "14394      5   1078    568  sobaek.tif          13           QV   test\n",
       "14395      5   1126    505  sobaek.tif          14           QV   test\n",
       "14396      5    190    430  sobaek.tif          11           QV   test\n",
       "14397      5   3535    751  sobaek.tif          30           QV   test\n",
       "14398      5   1393    244  sobaek.tif           4           QV   test\n",
       "\n",
       "[242500 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/label_data/species/label_mapping_concated.csv\", encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242500, 228101, 14399, 242500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df),len(df_train),len(df_test), len(df_train) + len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "patience=None\n",
    "bands = 10\n",
    "patch_size = 9\n",
    "time_idx = 4 #5ì›”ì´ˆ\n",
    "large_tif_dir = '../../data/source_data/with_s2' #ì›ì²œë°ì´í„° ì£¼ì†Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TiffDataset(\n\u001b[1;32m      2\u001b[0m             large_tif_dir \u001b[38;5;241m=\u001b[39m large_tif_dir,\n\u001b[1;32m      3\u001b[0m             file_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_1.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_2.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msobaek.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m#ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m             label_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/label_data/species/label_mapping_sampled.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m             box_filter_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m box_number: \u001b[38;5;129;01mnot\u001b[39;00m val_filter(box_number),\n\u001b[1;32m      6\u001b[0m             patch_size \u001b[38;5;241m=\u001b[39m patch_size,\n\u001b[1;32m      7\u001b[0m             transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TiffDataset(\n\u001b[1;32m     11\u001b[0m             large_tif_dir \u001b[38;5;241m=\u001b[39m large_tif_dir,\n\u001b[1;32m     12\u001b[0m             file_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_1.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_2.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msobaek.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m#ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m             transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/my_study/forest_carbon_ai/model/classification_2/utils/utils.py:327\u001b[0m, in \u001b[0;36mTiffDataset.__init__\u001b[0;34m(self, large_tif_dir, file_list, label_file, patch_size, box_filter_fn, transform)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_filter_fn \u001b[38;5;241m=\u001b[39m box_filter_fn \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28;01mlambda\u001b[39;00m box_number: \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_filter_fn)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m=\u001b[39m patch_size\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhalf_size \u001b[38;5;241m=\u001b[39m patch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/A/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/A/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/A/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/A/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m~/anaconda3/envs/A/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(box_number)\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TiffDataset(\n\u001b[1;32m      2\u001b[0m             large_tif_dir \u001b[38;5;241m=\u001b[39m large_tif_dir,\n\u001b[1;32m      3\u001b[0m             file_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_1.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_2.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msobaek.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m#ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m             label_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/label_data/species/label_mapping_sampled.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m             box_filter_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m box_number: \u001b[38;5;129;01mnot\u001b[39;00m val_filter(box_number),\n\u001b[1;32m      6\u001b[0m             patch_size \u001b[38;5;241m=\u001b[39m patch_size,\n\u001b[1;32m      7\u001b[0m             transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m      8\u001b[0m         )\n\u001b[1;32m     10\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TiffDataset(\n\u001b[1;32m     11\u001b[0m             large_tif_dir \u001b[38;5;241m=\u001b[39m large_tif_dir,\n\u001b[1;32m     12\u001b[0m             file_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_1.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjiri_2.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msobaek.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m#ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m             transform\u001b[38;5;241m=\u001b[39mtransform\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_filter' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = TiffDataset(\n",
    "            large_tif_dir = large_tif_dir,\n",
    "            file_list = [\"jiri_1.tif\", \"jiri_2.tif\", \"sobaek.tif\"], #ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\n",
    "            label_file = \"../../data/label_data/species/label_mapping_sampled.csv\",\n",
    "            box_filter_fn = lambda box_number: not val_filter(box_number),\n",
    "            patch_size = patch_size,\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "val_dataset = TiffDataset(\n",
    "            large_tif_dir = large_tif_dir,\n",
    "            file_list = [\"jiri_1.tif\", \"jiri_2.tif\", \"sobaek.tif\"], #ì „ì²´ ì§€ì—­ì„ ëª¨ë‘ ì‚¬ìš©í•œë‹¤.\n",
    "            label_file =\"../../data/label_data/species/label_mapping_sampled.csv\",\n",
    "            box_filter_fn = val_filter,\n",
    "            patch_size = patch_size,\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     base_transform(bands, patch_size),\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[:,time_idx])\n\u001b[1;32m      4\u001b[0m ])\n\u001b[0;32m----> 6\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m      7\u001b[0m val_dataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    base_transform(bands, patch_size),\n",
    "    transforms.Lambda(lambda x: x[:,time_idx])\n",
    "])\n",
    "\n",
    "train_dataset.transform = transform\n",
    "val_dataset.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetClassifier(in_channels=10).to(device)\n",
    "model.load_state_dict(torch.load(f\"./checkpoints/spectral/resnet_{bands}_{patch_size}_{kernel_size}_{num_epochs}.pth\"))\n",
    "# ğŸ”¹ Loss function (Categorical Classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ğŸ”¹ Optimizer (AdamW with weight decay)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# # ğŸ”¹ Learning Rate Scheduler (StepLR: 10 epochsë§ˆë‹¤ lr 0.1ë°° ê°ì†Œ)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ (í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì™€ í•™ìŠµ ë¶ˆê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í¬í•¨)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ (requires_grad == Trueì¸ íŒŒë¼ë¯¸í„°)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜:\", total_params)\n",
    "print(\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=patience)\n",
    "torch.save(best_model_state, os.path.join(checkpoints_dir, f\"resnet_{bands}_{patch_size}_{kernel_size}_{num_epochs}.pth\"))\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"\\ntrain data\")\n",
    "evaluate_model_with_cm(model, train_loader, num_classes=6)\n",
    "print(\"\\nvalidation data\")\n",
    "evaluate_model_with_cm(model, val_loader, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_transform(x):\n",
    "    x = x.squeeze(-1).squeeze(-1)  # (10, 12, 1, 1) â†’ (10, 12)\n",
    "    x = x.permute(1, 0)  # (10, 12) â†’ (12, 10)\n",
    "    return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    base_transform(bands, patch_size),\n",
    "    DownsampleWithGaussian(kernel_size, sigma=3),\n",
    "    transforms.Lambda(reshape_transform)\n",
    "])\n",
    "\n",
    "train_dataset.transform = transform\n",
    "val_dataset.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  TransformerClassifier(input_dim=10, embed_dim=16, num_heads=2, num_layers=2, num_classes=6, seq_len=12).to(device)\n",
    "\n",
    "# ğŸ”¹ Loss function (Categorical Classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer ì„¤ì •\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=patience)\n",
    "torch.save(best_model_state, os.path.join(checkpoints_dir, f\"transformer_{bands}_{patch_size}_{num_epochs}.pth\"))\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "print(\"\\ntrain data\")\n",
    "evaluate_model_with_cm(model, train_loader, num_classes=6)\n",
    "print(\"\\nvalidation data\")\n",
    "evaluate_model_with_cm(model, val_loader, num_classes=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A",
   "language": "python",
   "name": "a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
